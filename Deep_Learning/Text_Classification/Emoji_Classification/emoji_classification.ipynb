{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7099bd87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c49f6d39c4a4a59899e54acd5d697e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ffae374e3ed4c2cb75d1414647f499e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/1.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159e8eb06e7a4c09babd2f1fe900f6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/127k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806a9d5fa510412b90bb9c5ca695e742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/129k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da9f70c04dc4d9989706e10fec6dd02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9773a3522242400981aae8f0377067ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403aca6c613d4f9c8fbd63534c515f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"emotion\")\n",
    "\n",
    "# Split train/test\n",
    "train_texts = dataset['train']['text']\n",
    "train_labels = dataset['train']['label']\n",
    "test_texts = dataset['test']['text']\n",
    "test_labels = dataset['test']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da531ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee5511f71604931b6c3b2e192ff2b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dde005850424d82ad79afbb860d8b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a15e4348d0d41309656606ae9130355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d15f0a3aa34eed8cccd552f29147d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def tokenize(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "train_encodings = tokenize(train_texts)\n",
    "test_encodings = tokenize(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd455cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for Emotion Classification\n",
    "\n",
    "    A PyTorch Dataset class that handles emotion text data and their corresponding labels.\n",
    "    It wraps the tokenized text encodings and emotion labels for use with DataLoader.\n",
    "\n",
    "    Args:\n",
    "        encodings (BatchEncoding): The tokenized text encodings from a BERT tokenizer\n",
    "        labels (list): List of emotion labels corresponding to the text samples\n",
    "\n",
    "    Attributes:\n",
    "        encodings (BatchEncoding): Stored tokenized text encodings\n",
    "        labels (Tensor): Emotion labels converted to torch tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a single sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve\n",
    "\n",
    "        Returns:\n",
    "            tuple: A dictionary containing the encoded text and its corresponding label\n",
    "        \"\"\"\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}, self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get the total number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = EmotionDataset(train_encodings, train_labels)\n",
    "test_dataset = EmotionDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "445a39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c490eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes):\n",
    "        \"\"\"Initialize the Emotion Classifier model.\n",
    "\n",
    "        A simple neural network architecture for emotion classification using word embeddings\n",
    "        and 1D convolution. The model consists of an embedding layer, followed by a 1D \n",
    "        convolution, ReLU activation, global average pooling, and a final linear layer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary from the tokenizer\n",
    "            embedding_dim (int): Dimension of the word embeddings\n",
    "            num_classes (int): Number of emotion classes to predict\n",
    "\n",
    "        Attributes:\n",
    "            embedding (nn.Embedding): Word embedding layer\n",
    "            conv1d (nn.Conv1d): 1D convolutional layer\n",
    "            relu (nn.ReLU): ReLU activation function\n",
    "            global_avg_pool (nn.AdaptiveAvgPool1d): Global average pooling layer\n",
    "            fc (nn.Linear): Final linear layer for classification\n",
    "        \"\"\"\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv1d = nn.Conv1d(in_channels=embedding_dim,\n",
    "                                out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of token IDs with shape (batch_size, sequence_length)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Logits for each emotion class with shape (batch_size, num_classes)\n",
    "\n",
    "        The forward pass performs the following operations:\n",
    "        1. Convert token IDs to embeddings\n",
    "        2. Permute dimensions for conv1d operation\n",
    "        3. Apply 1D convolution\n",
    "        4. Apply ReLU activation\n",
    "        5. Apply global average pooling\n",
    "        6. Squeeze unnecessary dimension\n",
    "        7. Apply final linear layer\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)           # (batch_size, seq_len, embedding_dim)\n",
    "        x = x.permute(0, 2, 1)         # (batch_size, embedding_dim, seq_len)\n",
    "        x = self.conv1d(x)             # (batch_size, 64, seq_len)\n",
    "        x = self.relu(x)               # (batch_size, 64, seq_len)\n",
    "        x = self.global_avg_pool(x)    # (batch_size, 64, 1)\n",
    "        x.squeeze_(2)                  # (batch_size, 64)\n",
    "        x = self.fc(x)                 # (batch_size, num_classes)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b6e7c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.4640\n",
      "Epoch 2/5, Loss: 0.7608\n",
      "Epoch 3/5, Loss: 0.3618\n",
      "Epoch 4/5, Loss: 0.2132\n",
      "Epoch 5/5, Loss: 0.1414\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 128\n",
    "num_classes = len(set(train_labels))\n",
    "model = EmotionClassifier(vocab_size, embedding_dim, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch, labels in train_loader:\n",
    "        batch = {key: val.to(device) for key, val in batch.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch['input_ids'])\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2641e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I am so excited about this new adventure!\n",
      "Predicted Emotion: joy\n"
     ]
    }
   ],
   "source": [
    "def predict(sentence):\n",
    "    \"\"\"Predicts the emotion for a given text sentence using the trained EmotionClassifier model.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input text sentence for emotion prediction.\n",
    "\n",
    "    Returns:\n",
    "        str: The predicted emotion label from the set ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'].\n",
    "             Returns \"Unknown emotion\" if the predicted index is out of range.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    1. Sets model to evaluation mode\n",
    "    2. Tokenizes the input sentence\n",
    "    3. Moves tensors to the appropriate device\n",
    "    4. Makes prediction using the model\n",
    "    5. Maps the predicted class index to emotion label\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    encoded = tokenize([sentence])\n",
    "    encoded = {key: val.to(device) for key, val in encoded.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(encoded['input_ids'])\n",
    "        predicted = torch.argmax(output, dim=1).cpu().item()\n",
    "\n",
    "    # Correct emotion mapping based on the \"emotion\" dataset labels\n",
    "    emotions = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "\n",
    "    try:\n",
    "        return emotions[predicted]\n",
    "    except IndexError:\n",
    "        return \"Unknown emotion\"\n",
    "\n",
    "\n",
    "# Test the prediction\n",
    "sample_sentence = \"I am so excited about this new adventure!\"\n",
    "print(f\"Input: {sample_sentence}\")\n",
    "print(f\"Predicted Emotion: {predict(sample_sentence)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
